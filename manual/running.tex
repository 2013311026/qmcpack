\chapter{Running QMCPACK}
\label{chap:running}

\section{Command line options}
\label{sec:commandline}


\section{Input files}
\label{sec:inputs}

\section{Output files}
\label{sec:outputs}

  scalar.dat\\
  dmc.dat\\
  stat.h5\\
  config.h5

\section{Running in parallel}
\label{sec:parallelrunning}

%considerations for mpi, threads, gpu.

\subsection{MPI}
QMCPACK is fully parallelized with MPI. When performing an ensemble job, all the MPI ranks are first equally divided into groups which perform individual QMC calculations. Within one calculation, all the walkers are fully distributed across all the MPI ranks in the group. Since MPI requires distributed memory, there must be at least one MPI per node. To maximize the efficiency, more facts should be taken into account. When using MPI+threads on compute nodes with more than one NUMA domain (e.g., AMD Interlagos CPU on Titan or a node with multiple CPU sockets), it is recommended to place as many MPI ranks as the number of NUMA domains if the memory is sufficient. On clusters with more than just one GPU per node (NVIDIA Tesla K80), it requires to use the same number of MPI ranks as the number of GPUs per node in order to let each MPI rank take one GPU.

\subsection{Use of OpenMP threads}
\label{sec:openmprunning}
Modern processors integrate multiple identical cores even with hardware threads on a single die to increase the total performance and maintain a reasonable power draw. 
QMCPACK takes advantage of all that compute capability on a processor by using threads via OpenMP programming model as well as threaded linear algebra libraries. By default, QMCPACK is always built with OpenMP enabled. When launching calculations, users should instruct QMCPACK to create the right number of threads per MPI rank by specifying environmental variable OMP\_NUM\_THREADS. Even in the GPU accelerated version, using threads significantly reduces the time spent on the calculations performed on CPU.

\subsubsection{Performance consideration}
As walkers are the basic units of workload in QMC algorithms, they are loosely coupled and distributed across all the threads. For this reason, the best strategy to run QMCPACK efficiently is to feed enough walkers to the available threads.

In a VMC calculation, the code automatically raises the actual number of walkers per MPI rank to the number of available threads if the user-specified number of walkers is smaller, see ``walkers/mpi=XXX'' in the VMC output. 
In a DMC calculation, the target number of walkers should be chosen to be slightly smaller than a multiple of the total number of available threads across all the MPI ranks belongs to this calculation. Since the number of walkers varies from generation to generation, its dynamical value should be slightly smaller or equal to that multiple most of the time.

\subsubsection{Memory consideration}
When using threads, some memory objects shared by all the threads. Usually these memory are read-only when the walkers are evolving, for instance the ionic distance table and wavefunction coefficients.
If a wavefunction is represented by B-splines, the whole table is shared by all the threads. It usually takes a large chunk of memory when a large primitive cell was used in the simulation. Its actual size is reported as ``MEMORY increase XXX MB BsplineSetReader'' in the output file. 
See details about how to reduce it in section~\ref{sec:splinebasis}.

The other memory objects which are distinct for each walker during random walk need to be associated with individual walkers and can not be shared. This part of memory grows linearly as the number of walkers per MPI rank. Those objects include wavefunction values (Slater determinants) at given electronic configurations and electron related distance tables (electron-electron distance table). Those matrices dominate the $N^2$ scaling of the memory usage per walker.

\subsection{Running on GPU machines}
\label{sec:gpurunning}

The GPU version on the NVIDIA CUDA platform is fully incorporated into
the main trunk. Currently some commonly used functionalities for
solid-state and molecular systems using B-spline single-particle
orbitals is supported. A detailed description of the GPU
implementation can be found in Ref. \cite{EslerKimCeperleyShulenburger2012}.

Current GPU implementation assumes one MPI process per
GPU. Vectorization is achieved over walkers, that is, all walkers are
propagated in parallel. In each GPU kernel, loops over electrons,
atomic cores or orbitals are further vectorized to exploit an
additional level of parallelism and to allow coalesced memory access.

%---------------------------------------------------------------------------%

\subsubsection{Supported GPU features}

\begin{enumerate}

  \item Quantum Monte Carlo methods:

    \begin{enumerate}
	\item Variational Monte Carlo (VMC).
	\item Diffusion Monte Carlo (DMC).
	\item Limited support for wavefunction optimization.
    \end{enumerate}

  \item Boundary conditions:

    \begin{enumerate}
	\item Periodic and open boundary conditions are fully supported.
	\item Twist-averaged boundary condition is supported for only real-valued wavefunctions.
	\item Mixed boundary conditions and complex wavefunctions (e.g. fixed phase) are not yet supported. 
    \end{enumerate}

  \item Wavefunctions:

    \begin{enumerate}
	\item Single Slater determinants with 3D B-spline orbitals. Only real-valued wavefunctions is supported, but tiling complex orbitals to supercells is supported as long as each k-point is a multiple of half a G-vector of the supercell.
	\item Mixed basis representation in which orbitals are represented as 1D splines times spherical harmonics in spherical regions (muffin tins) around atoms, and 3D B-splines in the interstitial region.
	\item One-body and two-body Jastrows represented as 1D B-splines are supported. Note that only single-precision arithmetic is fully functional at the time of writing. 
    \end{enumerate}

  \item Interaction types:

    \begin{enumerate}
	\item Semilocal (nonlocal and local) pseudopotentials.
	\item Coulomb interaction (electron-electron, electron-ion).
	\item Model periodic Coulomb (MPC) interaction.
    \end{enumerate}

\end{enumerate}

%---------------------------------------------------------------------------%

\subsubsection{Compiling the GPU code}

To build the executable \courier{qmcapp} with GPU support, follow
these steps:

\begin{enumerate}
  
  \item  Make sure NVIDIA's CUDA compiler, nvcc, is in the search
    path. In most cases, CMake should be able to locate the nvcc
    compiler on the system automatically.
  \item 
    \begin{enumerate}
      \item Run CMake with the argument \courier{QMC\_CUDA} switched on: \\
               \courier{
           	cd build \\
           	cmake -D QMC\_CUDA=1 .. \\
           	make}

       \item[] \hspace{-0.27in} or

       \item If a CMake toolchain file is used, switch on \courier{QMC\_CUDA} by
         including this line in the toolchain file: \\
               \courier{SET (QMC\_CUDA 1)} \\
                Then compile the code as before: \\
                \courier{
         	cd build \\
        	cmake -D CMAKE\_TOOLCHAIN\_FILE=[toolchain name] .. \\
         	make \\}
    \end{enumerate}

\end{enumerate}

%---------------------------------------------------------------------------%

\subsubsection{CMake variables for adjusting CUDA code build features}

These values can be changed by passing them as CMake's command line
options with the \courier{-D} flag, or using a toolchain file to
overwrite the default values. \\

\begin{enumerate}

\item \courier{QMC\_CUDA}

\begin{tabular}{l@{: }p{4.5in}}
\courier{=0} (default) & no GPU support, build QMCPACK as a CPU code \\
\courier{=1}               & build QMCPACK with GPU support \\
\end{tabular} \\

\item \courier{CUDA\_PRECISION}

\begin{tabular}{l@{: }p{4in}}
\courier{=float} (default) & single precision arithmetics and data
                             types will be used for GPU kernels \\
\courier{=double}           & double precision arithmetics and data
                                 types will be used for GPU
                                 kernels (Warning: not fully
                                 functional!) \\
\end{tabular}

\end{enumerate}

%----------------------------------------------------------------------------%

\subsubsection{Performance consideration}

The relative speedup of the GPU implementation increases with both the number of electrons and the number of walkers running on a GPU. Typically, 128-256 walkers per GPU utilize sufficient number of threads to operate the GPU efficiently and to hide memory-access latency. 

To achieve better performance, current implementation utilizes single precision operations on most GPU calculations, except for matrix inversions where double precision is required to retain high accuracy. The single precision GPU code is as accurate as the double precision CPU code up to a certain system size. Cross checking and verification of accuracy are encouraged for systems with more than approximately 1500 electrons.

%------------------------------------------------------------------------------%

\subsubsection{Memory consideration}

In the GPU implementation, each walker has an anonymous buffer on the GPU's global memory to store temporary data associated with the wavefunctions. Therefore, the amount of memory available on a GPU limits the number of walkers and eventually the system size that it can process.

If the GPU memory is exhausted, reduce the number of walkers per GPU. Coarsening the grids of the B-splines representation (by decreasing the value of meshfactor in the input file) can also lower the memory usage, at the expense (risk) of obtaining inaccurate results. Proceed with caution if this option has to be considered.

