//////////////////////////////////////////////////////////////////////
// This file is distributed under the University of Illinois/NCSA Open Source
// License.  See LICENSE file in top directory for details.
//
// Copyright (c) 2016 Jeongnim Kim and QMCPACK developers.
//
// File developed by:
// Miguel A. Morales, moralessilva2@llnl.gov 
//    Lawrence Livermore National Laboratory 
//
// File created by:
// Miguel A. Morales, moralessilva2@llnl.gov 
//    Lawrence Livermore National Laboratory 
////////////////////////////////////////////////////////////////////////////////

#include <vector>
#include <map>
#include <string>
#include <iostream>
#include <tuple>
#include<mutex>

#include "AFQMC/config.h"
#include "AFQMC/Numerics/csr_blas.hpp"

//#include "AFQMC/Wavefunctions/NOMSD.h"

namespace qmcplusplus
{

namespace afqmc
{

  /*
   * Calculates the local energy and overlaps of all the walkers in the set and 
   * returns them in the appropriate data structures
  */
  template<class WlkSet, class Mat, class TVec>
  void NOMSD::Energy_shared(const WlkSet& wset, Mat&& E, TVec&& Ov)
  {
    size_t nt = wset.size()*(1+dm_size(false));
    if(shmbuff_for_E.num_elements() < nt)
      shmbuff_for_E.reextent(iextensions<1u>{nt}); 
    assert(E.dimensionality==2);
    assert(Ov.dimensionality==1);
    assert(E.size(0)==wset.size()); 
    assert(E.stride(0)==E.size(1)); 
    assert(Ov.size(0)==wset.size()); 
    assert(E.size(1)==3); 

    // temporary runtime check for incompatible memory spaces
    {
      auto dev_ptr(make_device_ptr(E.origin()));
      auto dev_ptr_(make_device_ptr(Ov.origin()));
    }

    ComplexType zero(0.0);
    auto Gsize = dm_size(false);
    auto nwalk = wset.size();
    int nr=Gsize,nc=nwalk;
    if(transposed_G_for_E_) std::swap(nr,nc);
    CMatrix_ref G(make_device_ptr(shmbuff_for_E.origin()),{nr,nc});
    CVector_ref ov_(G.origin()+G.num_elements(),iextensions<1u>{nwalk});
    if(eloc2.size(0) != nwalk || eloc2.size(1) != 3) eloc2.reextent({nwalk,3}); 

    using std::fill_n;
    fill_n(Ov.origin(),nwalk,zero);
    fill_n(E.origin(),3*nwalk,zero);
            
    for(int nd=0; nd<ci.size(); nd++) {
      MixedDensityMatrix_for_E(wset,G,ov_,nd);
      ma::axpy(std::conj(ci[nd]),ov_,Ov);
      HamOp.energy(eloc2,G,nd,TG.TG_local().root());  
      //TG.TG_local().all_reduce_in_place_n(eloc2.origin(),3*nwalk,std::plus<>());
      for(int i=0; i<nwalk; ++i) 
        for(int k=0; k<3; ++k) 
// TO GPU
          E[i][k] += std::conj(ci[nd])*ov_[i]*eloc2[i][k];  
    }     
    TG.TG_local().all_reduce_in_place_n(to_address(E.origin()),3*nwalk,std::plus<>());
    for(int i=0; i<nwalk; ++i) 
      for(int k=0; k<3; k++) 
// TO GPU
        E[i][k] /= Ov[i]; 
    TG.local_barrier();
  }

  /*
   * Calculates the local energy and overlaps of all the walkers in the set and 
   * returns them in the appropriate data structures
   */
  template<class WlkSet, class Mat, class TVec>
  void NOMSD::Energy_distributed_singleDet(const WlkSet& wset, Mat&& E, TVec&& Ov)
  {
    //1. Calculate G and overlaps
    //2. Loop over nodes in TG
    // 2.a isend G to next node. irecv next G from "previous" node 
    // 2.b add local contribution to current G
    // 2.c wait for comms to finish
    //3. all reduce resulting energies    

    using std::fill_n;
    using std::copy_n;
    // temporary runtime check for incompatible memory spaces
    {
      auto dev_ptr(make_device_ptr(E.origin()));
      auto dev_ptr_(make_device_ptr(Ov.origin()));
    }
    assert(ci.size()==1);
    bool new_shm_space=false; 
    const int node_number = TG.getLocalNodeNumber();
    const int nnodes = TG.getNNodesPerTG();
    const int Gsize = dm_size(false);
    const ComplexType zero(0.0);
    const int nwalk = wset.size();
    // allocte space in shared memory for:
    //  i.  2 copies of G (always compact),
    //  ii. ovlps for local walkers
    //  iii. energies[3] for all walkers on all nodes of TG (assume all nodes have same # of walkers)
    int nt = nwalk*(2*Gsize+1);
    // in case the number of walkers changes
    if(shmbuff_for_E.num_elements() < nt) {
      shmbuff_for_E.reextent(iextensions<1u>{nt});
      new_shm_space=true;
    }
    assert(E.dimensionality==2);
    assert(Ov.dimensionality==1);
    assert(E.size(0)==wset.size());
    assert(Ov.size(0)==wset.size());
    assert(E.size(1)==3);

    int nr=Gsize,nc=nwalk;
    if(transposed_G_for_E_) std::swap(nr,nc);
    CMatrix_ref Gwork(make_device_ptr(shmbuff_for_E.origin()),{nr,nc});
    CMatrix_ref Grecv(Gwork.origin()+Gwork.num_elements(),{nr,nc});
    CVector_ref overlaps(Grecv.origin()+Grecv.num_elements(),iextensions<1u>{nwalk});
    if(eloc2.size(0) != nnodes*nwalk || eloc2.size(1) != 3) 
        eloc2.reextent({nnodes*nwalk,3});
    auto elocal = eloc2[node_number*nwalk];
    int nak0,nak1;
    std::tie(nak0,nak1) = FairDivideBoundary(TG.getLocalTGRank(),Gsize*nwalk,TG.getNCoresPerTG());

    if(new_shm_space) {
      // use mpi3 when ready
      if(req_Grecv!=MPI_REQUEST_NULL)
          MPI_Request_free(&req_Grecv);
      if(req_Gsend!=MPI_REQUEST_NULL)
          MPI_Request_free(&req_Gsend);
      MPI_Send_init(to_address(Gwork.origin())+nak0,(nak1-nak0)*sizeof(ComplexType),MPI_CHAR,
                    TG.prev_core(),1234,&(TG.TG()),&req_Gsend);
      MPI_Recv_init(to_address(Grecv.origin())+nak0,(nak1-nak0)*sizeof(ComplexType),MPI_CHAR,
                    TG.next_core(),1234,&(TG.TG()),&req_Grecv);
    }
   
    fill_n(eloc2.origin(),3*nnodes*nwalk,ComplexType(0.0)); 
    TG.local_barrier();

    MPI_Status st;

    // calculate G for local walkers
    MixedDensityMatrix_for_E(wset,Gwork,overlaps,0);

    for(int k=0; k<nnodes; k++) {

      // wait for G from node behind you, copy to Gwork  
      if(k>0) {
        MPI_Wait(&req_Grecv,&st);
        MPI_Wait(&req_Gsend,&st);     // need to wait for Gsend in order to overwrite Gwork  
        copy_n(Grecv.origin()+nak0,(nak1-nak0),Gwork.origin()+nak0);
        TG.local_barrier();
      }

      // post send/recv messages with nodes ahead and behind you
      if(k < nnodes-1) {
        MPI_Start(&req_Gsend); 
        MPI_Start(&req_Grecv); 
      }

      // calculate your contribution of the local enery to the set of walkers in Gwork
      int q = (k+node_number)%nnodes;
      HamOp.energy(eloc2.sliced(q*nwalk,(q+1)*nwalk),
                   Gwork,0,TG.TG_local().root() && k==0);
      TG.local_barrier();

    }
    TG.TG().all_reduce_in_place_n(to_address(eloc2.origin()),3*nnodes*nwalk,std::plus<>());
    TG.local_barrier();    
    copy_n(elocal.origin(),3*nwalk,E.origin());
    copy_n(overlaps.origin(),nwalk,Ov.origin());
    TG.local_barrier();    
  }

  /*
   * Calculates the local energy and overlaps of all the walkers in the set and 
   * returns them in the appropriate data structures
   * NOTE: This version assumes balanced partition of all the determinants in the list
   * Depending on the number of determinants and the number of nodes in TG, it may be better
   * to keep entire determinants on nodes and distribute just the excess ones.
   */
  template<class WlkSet, class Mat, class TVec>
  void NOMSD::Energy_distributed_multiDet(const WlkSet& wset, Mat&& E, TVec&& Ov)
  {
    //1. Copies SM from wset to shm buffer. 
    //2. Loop over nodes in TG
    // 2.a isend SM to next node. irecv next SM from "previous" node 
    // 2.b Calculate G and add local contribution to energy
    // 2.c wait for comms to finish
    //3. all reduce resulting energies    

    using std::fill_n;
    using std::copy_n;
    // temporary runtime check for incompatible memory spaces
    {
      auto dev_ptr(make_device_ptr(E.origin()));
      auto dev_ptr_(make_device_ptr(Ov.origin()));
    }
    assert(ci.size()>1);
    bool new_shm_space=false; 
    const int node_number = TG.getLocalNodeNumber();
    const int nnodes = TG.getNNodesPerTG();
    const int Gsize = dm_size(false); // this will also be the SlaterMat size
    const ComplexType zero(0.0);
    const int nwalk = wset.size();
    // allocte space in shared memory for:
    //  i.  2 copies of SM, 
    //  ii.  1 copy of G (always compact),
    //  iii. ovlps for local walkers
    int nt = nwalk*(3*Gsize+1);
    // in case the number of walkers changes
    if(shmbuff_for_E.num_elements() < nt) {
      shmbuff_for_E.reextent(iextensions<1u>{nt}); 
      new_shm_space=true;
    }
    assert(E.dimensionality==2);
    assert(Ov.dimensionality==1);
    assert(E.size(0)==wset.size());
    assert(Ov.size(0)==wset.size());
    assert(E.size(1)==3);

    int nr=Gsize,nc=nwalk;
    if(transposed_G_for_E_) std::swap(nr,nc);
    CMatrix_ref SMwork(make_device_ptr(shmbuff_for_E.origin()),{nwalk,Gsize});
    CMatrix_ref SMrecv(SMwork.origin()+SMwork.num_elements(),{nwalk,Gsize});
    CMatrix_ref Gwork(SMrecv.origin()+SMrecv.num_elements(),{nr,nc});
    CVector_ref overlaps(Gwork.origin()+Gwork.num_elements(),iextensions<1u>{nwalk});
    // used for temporary storage in ndet loop for local calculation
    if(eloc3.size(0) != nwalk || eloc3.size(1) != 3) 
        eloc3.reextent({nwalk,3});
    // used for temporary storage of numerator in energy expression for all walkers in TG
    if(eloc2.size(0) != nnodes*nwalk || eloc2.size(1) != 3) 
        eloc2.reextent({nnodes*nwalk,3});
    // matrix view to local segment of eloc2, for wasy access later
    // split SM evenly for communication
    int nak0,nak1;
    std::tie(nak0,nak1) = FairDivideBoundary(TG.getLocalTGRank(),Gsize*nwalk,TG.getNCoresPerTG());

    if(new_shm_space) {
      if(req_SMrecv!=MPI_REQUEST_NULL)
          MPI_Request_free(&req_SMrecv);
      if(req_SMsend!=MPI_REQUEST_NULL)
          MPI_Request_free(&req_SMsend);
      // use mpi3 when ready
      MPI_Send_init(to_address(SMwork.origin())+nak0,(nak1-nak0)*sizeof(ComplexType),MPI_CHAR,
                    TG.prev_core(),2345,&(TG.TG()),&req_SMsend);
      MPI_Recv_init(to_address(SMrecv.origin())+nak0,(nak1-nak0)*sizeof(ComplexType),MPI_CHAR,
                    TG.next_core(),2345,&(TG.TG()),&req_SMrecv);
    }
   
    fill_n(eloc2.origin(),3*nnodes*nwalk,ComplexType(0.0)); 
    fill_n(Ov.origin(),nwalk,ComplexType(0));
    if(TG.TG_local().root())
      fill_n(overlaps.origin(),nwalk,ComplexType(0));

    MPI_Status st;

    // copy SM from wset. Assumes that SM data is contiguous in memory (Alpha+Beta)
    for(int i=0; i<nwalk; i++) 
      if( i%TG.TG_local().size() == TG.TG_local().rank() )
        copy_n(wset[i].SlaterMatrix(Alpha).origin(),Gsize,SMwork[i].origin());
    TG.local_barrier();

    for(int k=0; k<nnodes; k++) {

      // wait for G from node behind you, copy to Gwork  
      if(k>0) {
        MPI_Wait(&req_SMrecv,&st);
        MPI_Wait(&req_SMsend,&st);     // need to wait for Gsend in order to overwrite Gwork  
        copy_n(SMrecv.origin()+nak0,(nak1-nak0),SMwork.origin()+nak0);
        TG.local_barrier();
      }

      // post send/recv messages with nodes ahead and behind you
      if(k < nnodes-1) {
        MPI_Start(&req_SMsend); 
        MPI_Start(&req_SMrecv); 
      }

      // calculate your contribution of the local enery to the set of walkers in Gwork
      int q = (k+node_number)%nnodes;
      for(int nd=0; nd<ci.size(); nd++) {
        MixedDensityMatrix_for_E_from_SM(SMwork,Gwork,overlaps,nd);
        if(k==0)  
          ma::axpy(std::conj(ci[nd]),overlaps,Ov);
        HamOp.energy(eloc3,Gwork,nd,TG.TG_local().root()&&k==0);
        for(int i=0; i<nwalk; ++i) 
          for(int k=0; k<3; k++)
// TO GPU
            eloc2[q*nwalk+i][k] += std::conj(ci[nd])*overlaps[i]*eloc3[i][k];
      }
      TG.local_barrier();

    }

    TG.TG().all_reduce_in_place_n(to_address(eloc2.origin()),3*nnodes*nwalk,std::plus<>());
    TG.local_barrier();    
    auto elocal = eloc2.sliced(node_number*nwalk,(node_number+1)*nwalk);
    for(int i=0; i<nwalk; i++) {
// TO GPU
      E[i][0] = elocal[i][0]/Ov[i];
      E[i][1] = elocal[i][1]/Ov[i];
      E[i][2] = elocal[i][2]/Ov[i];
    }
    TG.local_barrier();    
  }

  /* 
   * Computes the mixed density matrix of a single given determinant in the trial wave function.
   * Intended to be used in combination with the energy evaluation routine.
   * G and Ov are expected to be in shared memory.
   * Simple round-robin is used. 
   */
  template<class MatSM, class MatG, class TVec>
  void NOMSD::MixedDensityMatrix_for_E_from_SM(const MatSM& SM, MatG&& G, TVec&& Ov, int nd) 
  {
    using std::fill_n;
    using std::copy_n;
    auto Gsize = size_t(dm_size(false));
    const int nw = SM.size(0); 
    assert(G.stride(1)==1);
    assert(Ov.stride(0)==1);
    if(transposed_G_for_E_) 
      assert(G.size(0) == nw && G.size(1) == size_t(dm_size(false)));
    else
      assert(G.size(1) == nw && G.size(0) == size_t(dm_size(false)));
    assert(Ov.size() >= nw);  
    assert(SM.size(1) == Gsize);
    // to force synchronization before modifying structures in SHM
    TG.local_barrier();
    if(TG.TG_local().root())
      std::fill_n(Ov.origin(),nw,0);
    for(int i=0; i<G.size(0); i++)
      if( i%TG.TG_local().size() == TG.TG_local().rank() )
        std::fill_n(G[i].origin(),G.size(1),ComplexType(0.0));
    TG.local_barrier();
    if(localGbuff.size() < Gsize)
      localGbuff.reextent(iextensions<1u>{Gsize});
    if(walker_type != COLLINEAR) {

      auto Gdims = dm_dims(false,Alpha);
      CMatrix_ref G2D_(localGbuff.origin(),{Gdims.first,Gdims.second});
      CVector_ref G1D_(G2D_.origin(),iextensions<1u>{Gsize});
      // notice interchange of dimensions
      CTensor_cref A(SM.origin(),{nw,Gdims.second,Gdims.first});

      for(int iw=0; iw<nw; ++iw) {
        if(iw%TG.TG_local().size() != TG.TG_local().rank()) continue;
        SDetOp.MixedDensityMatrix(OrbMats[nd],A[iw],G2D_,to_address(Ov.origin())+iw,true);
        if(walker_type==CLOSED) Ov[iw] *= ComplexType(Ov[iw]);
        if(transposed_G_for_E_) 
          G[iw] = G1D_; 
        else
          G(G.extension(0),iw) = G1D_; 
      }

    } else {

      // store overlaps locally to be able to split alpha/beta pairs
      if(ovlp2.size(0) < 2*nw) ovlp2.reextent(iextensions<1u>{2*nw});
      fill_n(ovlp2.origin(),2*nw,ComplexType(0.0));
      auto GAdims = dm_dims(false,Alpha);
      auto GBdims = dm_dims(false,Beta);
      CMatrix_ref GA2D_(localGbuff.origin(),{GAdims.first,GAdims.second});
      CMatrix_ref GB2D_(GA2D_.origin()+GA2D_.num_elements(),
                                   {GBdims.first,GBdims.second});
      CVector_ref GA1D_(GA2D_.origin(),iextensions<1u>{GA2D_.num_elements()});
      CVector_ref GB1D_(GB2D_.origin(),iextensions<1u>{GB2D_.num_elements()});

      for(int iw=0; iw<2*nw; ++iw) {
        if(iw%TG.TG_local().size() != TG.TG_local().rank()) continue;
        
        if(iw%2==0) {
          // notice interchange of dimensions
          CMatrix_cref M(SM[iw/2].origin(),{GAdims.second,GAdims.first});
          SDetOp.MixedDensityMatrix(OrbMats[2*nd],M,GA2D_,to_address(ovlp2.origin())+iw,true);
          if(transposed_G_for_E_) 
            G[iw/2].sliced(0,GAdims.first*GAdims.second) = GA1D_;
          else
            G({0,GAdims.first*GAdims.second},iw/2) = GA1D_;
        } else {
          // notice interchange of dimensions
          CMatrix_cref M(SM[iw/2].origin()+GAdims.first*GAdims.second,
                                               {GBdims.second,GBdims.first});
          SDetOp.MixedDensityMatrix(OrbMats[2*nd+1],M,GB2D_,to_address(ovlp2.origin())+iw,true);
          if(transposed_G_for_E_) 
            G[iw/2].sliced(GAdims.first*GAdims.second,G.size(1)) = GB1D_;
          else
            G({GAdims.first*GAdims.second,G.size(0)},iw/2) = GB1D_;
        }
      }
      // CHECK: I don't need all_reduce here, but the current version of mpi3 
      //        fails if I use reduce_in_place_n  
      TG.TG_local().all_reduce_in_place_n(to_address(ovlp2.origin()),2*nw,std::plus<>());  
      if(TG.TG_local().root())
        for(int iw=0; iw<nw; ++iw)
// to GPU
          Ov[iw] = ovlp2[2*iw]*ovlp2[2*iw+1];
    }
    TG.local_barrier();
  }

  /* 
   * Computes the mixed density matrix of a single given determinant in the trial wave function.
   * Intended to be used in combination with the energy evaluation routine.
   * G and Ov are expected to be in shared memory.
   * Simple round-robin is used. 
   */
  template<class WlkSet, class MatG, class TVec>
  void NOMSD::MixedDensityMatrix_for_E(const WlkSet& wset, MatG&& G, TVec&& Ov, int nd) 
  {
    assert(G.stride(1)==1);
    assert(Ov.stride(0)==1);
    if(transposed_G_for_E_) 
      assert(G.size(0) == wset.size() && G.size(1) == size_t(dm_size(false)));
    else
      assert(G.size(1) == wset.size() && G.size(0) == size_t(dm_size(false)));
    const int nw = wset.size(); 
    assert(Ov.size() >= nw);  
    // to force synchronization before modifying structures in SHM
    TG.local_barrier();
    if(TG.TG_local().root())
      fill_n(Ov.origin(),nw,0);
    for(int i=0; i<G.size(0); i++)
      if( i%TG.TG_local().size() == TG.TG_local().rank() )
        fill_n(G[i].origin(),G.size(1),ComplexType(0.0));
    TG.local_barrier();
    auto Gsize = size_t(dm_size(false));
    if(localGbuff.size() < Gsize)
      localGbuff.reextent(iextensions<1u>{Gsize});
    if(walker_type != COLLINEAR) {

      auto Gdims = dm_dims(false,Alpha);
      CMatrix_ref G2D_(localGbuff.origin(),{Gdims.first,Gdims.second});
      CVector_ref G1D_(G2D_.origin(),iextensions<1u>{G2D_.num_elements()});

      for(int iw=0; iw<nw; ++iw) {
        if(iw%TG.TG_local().size() != TG.TG_local().rank()) continue;
        SDetOp.MixedDensityMatrix(OrbMats[nd],wset[iw].SlaterMatrix(Alpha),
                                                   G2D_,to_address(Ov.origin())+iw,true);
        if(walker_type==CLOSED) Ov[iw] *= ComplexType(Ov[iw]);
        if(transposed_G_for_E_) 
          G[iw] = G1D_; 
        else
          G(G.extension(0),iw) = G1D_;
      }

    } else {

      // store overlaps locally to be able to split alpha/beta pairs
      if(ovlp2.size(0) < 2*nw) ovlp2.reextent(iextensions<1u>{2*nw});
      fill_n(ovlp2.origin(),2*nw,ComplexType(0.0));
      auto GAdims = dm_dims(false,Alpha);
      auto GBdims = dm_dims(false,Beta);
      CMatrix_ref GA2D_(localGbuff.origin(),{GAdims.first,GAdims.second});
      CMatrix_ref GB2D_(GA2D_.origin()+GA2D_.num_elements(),
                                   {GBdims.first,GBdims.second});
      CVector_ref GA1D_(GA2D_.origin(),iextensions<1u>{GA2D_.num_elements()});
      CVector_ref GB1D_(GB2D_.origin(),iextensions<1u>{GB2D_.num_elements()});

      for(int iw=0; iw<2*nw; ++iw) {
        if(iw%TG.TG_local().size() != TG.TG_local().rank()) continue;
        
        if(iw%2==0) {
          SDetOp.MixedDensityMatrix(OrbMats[2*nd],wset[iw/2].SlaterMatrix(Alpha),
                                                   GA2D_,to_address(ovlp2.origin())+iw,true);
          if(transposed_G_for_E_) 
            G[iw/2].sliced(0,GAdims.first*GAdims.second) = GA1D_;
          else  
            G({0,GAdims.first*GAdims.second},iw/2) = GA1D_;
        } else {
          SDetOp.MixedDensityMatrix(OrbMats[2*nd+1],wset[iw/2].SlaterMatrix(Beta),
                                                   GB2D_,to_address(ovlp2.origin())+iw,true);
          if(transposed_G_for_E_) 
            G[iw/2].sliced(GAdims.first*GAdims.second,G.size(1)) = GB1D_;
          else  
            G({GAdims.first*GAdims.second,G.size(0)},iw/2) = GB1D_;
        }
      }
      // CHECK: I don't need all_reduce here, but the current version of mpi3 
      //        fails if I use reduce_in_place_n  
      TG.TG_local().all_reduce_in_place_n(to_address(ovlp2.origin()),2*nw,std::plus<>());  
      if(TG.TG_local().root())
        for(int iw=0; iw<nw; ++iw)
// to GPU
          Ov[iw] = ovlp2[2*iw]*ovlp2[2*iw+1];
    }
    TG.local_barrier();
  }

  /*
   * This routine has (potentially) considerable overhead if either the number of determinants
   *   or the number of walkers changes.   
   * G is assumed to be in shared memory
   * 1. calculate G(iw,nd) in a local buffer  
   * 2. accumulate the numerator in G with mutex
   * 3. 
   * Ov is assumed to be local to the core
   */ 
  template<class WlkSet, class MatG, class TVec>
  void NOMSD::MixedDensityMatrix(const WlkSet& wset, MatG&& G, TVec&& Ov, bool compact, bool transpose)
  {
    using std::fill_n;
    using std::copy_n;
    // temporary runtime check for incompatible memory spaces
    {
      auto dev_ptr_(make_device_ptr(Ov.origin()));
      auto dev_ptr(make_device_ptr(G.origin()));
    }

    assert(G.stride(1)==1);
    assert(Ov.stride(0)==1);
    if(transpose)
      assert(G.size(0) == wset.size() && G.size(1) == size_t(dm_size(not compact)));
    else
      assert(G.size(1) == wset.size() && G.size(0) == size_t(dm_size(not compact)));
    const int nw = wset.size(); 
    const int ndet = ci.size();
    assert(Ov.size() >= nw);  
    fill_n(Ov.origin(),nw,0);
    for(int i=0; i<G.size(0); i++) 
      if( i%TG.TG_local().size() == TG.TG_local().rank() ) 
        fill_n(G[i].origin(),G.size(1),ComplexType(0.0));
    TG.local_barrier();
    auto Gsize = size_t(dm_size(not compact));
    if(localGbuff.size() < Gsize+1)
      localGbuff.reextent(iextensions<1u>{Gsize+1});
    pointer ov_(localGbuff.origin()+Gsize);  
    auto wlk_dims = wset.walker_dims();
    if(walker_type != COLLINEAR) {

      auto Gdims = dm_dims(not compact,Alpha);
      CMatrix_ref G2D_(localGbuff.origin(),{Gdims.first,Gdims.second});
      CVector_ref G1D_(G2D_.origin(),iextensions<1u>{G2D_.num_elements()});

      const int ntasks_percore = (nw*ndet)/TG.getNCoresPerTG();
      const int ntasks_total_serial = ntasks_percore*TG.getNCoresPerTG();
      const int nextra = nw*ndet - ntasks_total_serial; 

      // each processor does ntasks_percore_serial overlaps serially
      const int tk0 = TG.getLocalTGRank()*ntasks_percore; 
      const int tkN = (TG.getLocalTGRank()+1)*ntasks_percore; 

      // task_w_d = = wlk_w*ndet + d 
      for(int tk=tk0; tk<tkN; ++tk) {
        int iw = tk/ndet;
        int nd = tk%ndet;
        SDetOp.MixedDensityMatrix(OrbMats[nd],wset[iw].SlaterMatrix(Alpha),
                                                   G2D_,to_address(ov_),compact);
// to GPU???
        ComplexType ov(*ov_);
        if(walker_type==CLOSED) ov *= ov;
        ov *= std::conj(ci[nd]);
        // if the overhead of the mutex is too much, invert the loop order (make iw the fast index)
        // and have a mutex for each walker index 
        if(transpose) {
          std::lock_guard<shared_mutex> guard(*mutex);
          ma::axpy(ov,G1D_,G[iw]);   
        } else {
          std::lock_guard<shared_mutex> guard(*mutex);
          ma::axpy(ov,G1D_,G(G.extension(0),iw));   
        }
        Ov[iw] += ov; 
      }

      // all remaining overlaps are performed in parallel with blocks of cores
      // partition processors in nextra groups
      if(nextra > 0) {
        // check if new communicator is necessary
        if( last_number_extra_tasks != nextra ) {
          last_number_extra_tasks = nextra;
          for(int n=0; n<nextra; n++) {
            int n0,n1;
            std::tie(n0,n1) = FairDivideBoundary(n,TG.getNCoresPerTG(),nextra);
            if(TG.getLocalTGRank()>=n0 && TG.getLocalTGRank()<n1) {
              last_task_index = n;
              break;
            }
          }   
          // first setup
          local_group_comm = std::move(shared_communicator(TG.TG_local().split(last_task_index,TG.TG_local().rank()))); 
          shmbuff_for_G = std::move(std::make_unique<mpi3CVector>(iextensions<1u>{dm_size(true)},
                                                shared_allocator<ComplexType>{local_group_comm}));
        } 
        if(last_task_index < 0 || last_task_index > nextra) 
          APP_ABORT("Error: Problems in NOMSD::Overlap(WSet,Ov)");
        {
          boost::multi::array_ref<ComplexType,2> G2D_2(to_address(shmbuff_for_G->origin()),
                                                 {Gdims.first,Gdims.second});
          boost::multi::array_ref<ComplexType,1> G1D_2(to_address(shmbuff_for_G->origin()),iextensions<1u>{Gsize});
          int iw = (last_task_index+ntasks_total_serial)/ndet;
          int nd = (last_task_index+ntasks_total_serial)%ndet;
          ComplexType ov;
          SDetOp.MixedDensityMatrix(OrbMats[nd],wset[iw].SlaterMatrix(Alpha),
                                         G2D_2,std::addressof(ov),local_group_comm,compact);
          if(walker_type==CLOSED) ov *= ov;
          ov *= std::conj(ci[nd]);

          if(local_group_comm.rank()==0) {
            if(transpose) {
              std::lock_guard<shared_mutex> guard(*mutex);
              ma::axpy(ov,G1D_2,G[iw]);
            } else {
              std::lock_guard<shared_mutex> guard(*mutex);
              ma::axpy(ov,G1D_2,G(G.extension(0),iw));
            }
            Ov[iw] += ov; 
          }
        }
      }

    } else {

      auto GAdims = dm_dims(not compact,Alpha);
      auto GBdims = dm_dims(not compact,Beta);
      CMatrix_ref GA2D_(localGbuff.origin(),{GAdims.first,GAdims.second});
      CMatrix_ref GB2D_(GA2D_.origin()+GA2D_.num_elements(),
                                   {GBdims.first,GBdims.second});
      CVector_ref GA1D_(GA2D_.origin(),iextensions<1u>{GA2D_.num_elements()});
      CVector_ref GB1D_(GB2D_.origin(),iextensions<1u>{GB2D_.num_elements()});

      const int ntasks_percore = (nw*ndet)/TG.getNCoresPerTG();
      const int ntasks_total_serial = ntasks_percore*TG.getNCoresPerTG();
      const int nextra = nw*ndet - ntasks_total_serial;

      // each processor does ntasks_percore_serial overlaps serially
      const int tk0 = TG.getLocalTGRank()*ntasks_percore;
      const int tkN = (TG.getLocalTGRank()+1)*ntasks_percore;


      // task_w_d = = wlk_w*ndet + d 
      for(int tk=tk0; tk<tkN; ++tk) {
        int iw = tk/ndet;
        int nd = tk%ndet;
        ComplexType ov;
        SDetOp.MixedDensityMatrix(OrbMats[2*nd],wset[iw].SlaterMatrix(Alpha),
                                        GA2D_,to_address(ov_),compact);
        ov = ComplexType(*ov_);
        SDetOp.MixedDensityMatrix(OrbMats[2*nd+1],wset[iw].SlaterMatrix(Beta),
                                         GB2D_,to_address(ov_),compact);
        ov *= ComplexType(*ov_)*std::conj(ci[nd]);
        if(transpose) {
          std::lock_guard<shared_mutex> guard(*mutex);
          ma::axpy(ov,GA1D_,G[iw].sliced(0,GAdims.first*GAdims.second));
          ma::axpy(ov,GB1D_,G[iw].sliced(GAdims.first*GAdims.second,G.size(1)));
        } else {
          std::lock_guard<shared_mutex> guard(*mutex);
          ma::axpy(ov,GA1D_,G({0,GAdims.first*GAdims.second},iw));
          ma::axpy(ov,GB1D_,G({GAdims.first*GAdims.second,G.size(0)},iw));
        }
        Ov[iw] += ov;
      }

      // all remaining overlaps are performed in parallel with blocks of cores
      // partition processors in nextra groups
      if(nextra > 0) {
        // check if new communicator is necessary
        if( last_number_extra_tasks != nextra ) {
          last_number_extra_tasks = nextra;
          for(int n=0; n<nextra; n++) {
            int n0,n1;
            std::tie(n0,n1) = FairDivideBoundary(n,TG.getNCoresPerTG(),nextra);
            if(TG.getLocalTGRank()>=n0 && TG.getLocalTGRank()<n1) {
              last_task_index = n;
              break;
            }
          }
          // first setup
          //local_group_comm = std::move(std::make_unique<shared_communicator>(TG.TG_local().split(last_task_index))); 
          local_group_comm = std::move(shared_communicator(TG.TG_local().split(last_task_index,TG.TG_local().rank()))); 
          shmbuff_for_G = std::move(std::make_unique<mpi3CVector>(iextensions<1u>{dm_size(true)},
                                                shared_allocator<ComplexType>{local_group_comm}));
        }
        if(last_task_index < 0 || last_task_index > nextra)
          APP_ABORT("Error: Problems in NOMSD::Overlap(WSet,Ov)");
        {
          boost::multi::array_ref<ComplexType,2> GA2D_2(to_address(shmbuff_for_G->origin()),
                               {GAdims.first,GAdims.second});
          boost::multi::array_ref<ComplexType,2> GB2D_2(to_address(shmbuff_for_G->origin())+
                               GAdims.first*GAdims.second,{GBdims.first,GBdims.second});
          boost::multi::array_ref<ComplexType,1> GA1D_2(to_address(shmbuff_for_G->origin()),
                               iextensions<1u>{GAdims.first*GAdims.second});
          boost::multi::array_ref<ComplexType,1> GB1D_2(to_address(shmbuff_for_G->origin())+
                               GAdims.first*GAdims.second,iextensions<1u>{GBdims.first*GBdims.second});
          int task = (last_task_index+ntasks_total_serial);
          int iw = task/ndet;
          int nd = task%ndet;
          ComplexType ov, ov2_;
          SDetOp.MixedDensityMatrix(OrbMats[2*nd],wset[iw].SlaterMatrix(Alpha),
                                    GA2D_2,to_address(ov_),local_group_comm,compact);
          ov = ComplexType(*ov_);
          SDetOp.MixedDensityMatrix(OrbMats[2*nd+1],wset[iw].SlaterMatrix(Beta),
                                           GB2D_2,to_address(ov_),local_group_comm,compact);
          ov *= ComplexType(*ov_)*std::conj(ci[nd]);
          if(local_group_comm.root()) {
            if(transpose) {
              std::lock_guard<shared_mutex> guard(*mutex);
              ma::axpy(ov,GA1D_2,G[iw].sliced(0,GAdims.first*GAdims.second));
              ma::axpy(ov,GB1D_2,G[iw].sliced(GAdims.first*GAdims.second,G.size(1)));
            } else {
              std::lock_guard<shared_mutex> guard(*mutex);
              ma::axpy(ov,GA1D_2,G({0,GAdims.first*GAdims.second},iw));
              ma::axpy(ov,GB1D_2,G({GAdims.first*GAdims.second,G.size(0)},iw));
            }
            Ov[iw] += ov;
          }
        }
      }
    }
    // normalize G
    TG.TG_local().all_reduce_in_place_n(to_address(Ov.origin()),nw,std::plus<>());
    if(transpose) {
      for(size_t iw=0; iw<G.size(0); ++iw) 
        if( iw%TG.TG_local().size() == TG.TG_local().rank() ) {
          auto ov = ComplexType(1.0,0.0)/Ov[iw];
// to GPU: write routine to do y[i,j] = y[i,j] op x[i], where op can be +,-,*,/
// operation_over_rows???, scale_rows???
          ma::scal(ov,G[iw]);
        }
    } else {
      auto Ov_ = Ov.origin();  
      const size_t nw_ = G.size(1);  
      for(int ik=0; ik<G.size(0); ++ik)
        if( ik%TG.TG_local().size() == TG.TG_local().rank() ) {
          auto Gik = G[ik].origin();  
          for(size_t iw=0; iw<nw_; ++iw)
            Gik[iw] /= Ov_[iw];
        }
    }
    TG.local_barrier();
  } 


  /*
   * Calculates the overlaps of all walkers in the set. Returns values in arrays. 
   * Ov is assumed to be local to the core
   */
  template<class WlkSet, class TVec>
  void NOMSD::Overlap(const WlkSet& wset, TVec&& Ov)
  {
    using std::fill_n;
    using std::copy_n;
    // temporary runtime check for incompatible memory spaces
    {
      auto dev_ptr_(make_device_ptr(Ov.origin()));
    }
    const int nw = wset.size(); 
    const int ndet = ci.size();
    assert(Ov.size() >= nw);  
    fill_n(Ov.origin(),nw,0);
    if(localGbuff.size() < 1)
      localGbuff.reextent(iextensions<1u>{1});
    pointer ov_(localGbuff.origin());
    if(walker_type != COLLINEAR) {

      const int ntasks_percore = (nw*ndet)/TG.getNCoresPerTG();
      const int ntasks_total_serial = ntasks_percore*TG.getNCoresPerTG();
      const int nextra = nw*ndet - ntasks_total_serial; 

      // each processor does ntasks_percore_serial overlaps serially
      const int tk0 = TG.getLocalTGRank()*ntasks_percore; 
      const int tkN = (TG.getLocalTGRank()+1)*ntasks_percore; 

      // task_w_d = = wlk_w*ndet + d 
      for(int tk=tk0; tk<tkN; ++tk) {
        int iw = tk/ndet;
        int nd = tk%ndet;
        SDetOp.Overlap(OrbMats[nd],wset[iw].SlaterMatrix(Alpha),to_address(ov_));
        ComplexType ov(*ov_);
        Ov[iw] += std::conj(ci[nd])*ov*((walker_type==CLOSED)?(ov):(ComplexType(1.0,0.0)));
      }

      // all remaining overlaps are performed in parallel with blocks of cores
      // partition processors in nextra groups
      if(nextra > 0) {
        // check if new communicator is necessary
        if( last_number_extra_tasks != nextra ) {
          last_number_extra_tasks = nextra;
          for(int n=0; n<nextra; n++) {
            int n0,n1;
            std::tie(n0,n1) = FairDivideBoundary(n,TG.getNCoresPerTG(),nextra);
            if(TG.getLocalTGRank()>=n0 && TG.getLocalTGRank()<n1) {
              last_task_index = n;
              break;
            }
          }   
          // first setup
          local_group_comm = std::move(shared_communicator(TG.TG_local().split(last_task_index,TG.TG_local().rank()))); 
          shmbuff_for_G = std::move(std::make_unique<mpi3CVector>(iextensions<1u>{dm_size(true)},
                                                shared_allocator<ComplexType>{local_group_comm}));
        } 
        if(last_task_index < 0 || last_task_index > nextra) 
          APP_ABORT("Error: Problems in NOMSD::Overlap(WSet,Ov)");
        {
          int iw = (last_task_index+ntasks_total_serial)/ndet;
          int nd = (last_task_index+ntasks_total_serial)%ndet;
          ComplexType ov;
          SDetOp.Overlap(OrbMats[nd],wset[iw].SlaterMatrix(Alpha),std::addressof(ov),local_group_comm);
          if(local_group_comm.rank()==0)
            Ov[iw] += std::conj(ci[nd])*ov*((walker_type==CLOSED)?(ov):(ComplexType(1.0,0.0)));
  
        }
      }

    } else {

      const int ntasks_percore = (nw*ndet)/TG.getNCoresPerTG();
      const int ntasks_total_serial = ntasks_percore*TG.getNCoresPerTG();
      const int nextra = nw*ndet - ntasks_total_serial;

      // each processor does ntasks_percore_serial overlaps serially
      const int tk0 = TG.getLocalTGRank()*ntasks_percore;
      const int tkN = (TG.getLocalTGRank()+1)*ntasks_percore;

      // task_w_d = = wlk_w*ndet + d 
      for(int tk=tk0; tk<tkN; ++tk) {
        int iw = tk/ndet;
        int nd = tk%ndet;
        SDetOp.Overlap(OrbMats[2*nd],wset[iw].SlaterMatrix(Alpha),to_address(ov_));
        ComplexType ov(*ov_);
        SDetOp.Overlap(OrbMats[2*nd+1],wset[iw].SlaterMatrix(Beta),to_address(ov_));
        Ov[iw] += std::conj(ci[nd])*ov*ComplexType(*ov_); 
      }

      // all remaining overlaps are performed in parallel with blocks of cores
      // partition processors in nextra groups
      if(nextra > 0) {
        // check if new communicator is necessary
        if( last_number_extra_tasks != nextra ) {
          last_number_extra_tasks = nextra;
          for(int n=0; n<nextra; n++) {
            int n0,n1;
            std::tie(n0,n1) = FairDivideBoundary(n,TG.getNCoresPerTG(),nextra);
            if(TG.getLocalTGRank()>=n0 && TG.getLocalTGRank()<n1) {
              last_task_index = n;
              break;
            }
          }
          // first setup
          local_group_comm = std::move(shared_communicator(TG.TG_local().split(last_task_index,TG.TG_local().rank()))); 
          shmbuff_for_G = std::move(std::make_unique<mpi3CVector>(iextensions<1u>{dm_size(true)},
                                                shared_allocator<ComplexType>{local_group_comm}));
        }
        if(last_task_index < 0 || last_task_index > nextra)
          APP_ABORT("Error: Problems in NOMSD::Overlap(WSet,Ov)");
        {
          int task = (last_task_index+ntasks_total_serial);
          int iw = task/ndet;
          int nd = task%ndet;
          ComplexType ov_, ov2_;
          SDetOp.Overlap(OrbMats[2*nd],wset[iw].SlaterMatrix(Alpha),
                                          std::addressof(ov_),local_group_comm);
          SDetOp.Overlap(OrbMats[2*nd+1],wset[iw].SlaterMatrix(Beta),
                                          std::addressof(ov2_),local_group_comm);
          ov_ *= ov2_;
          if(local_group_comm.root())
            Ov[iw] += std::conj(ci[nd])*ov_; 
        }
      }

    }
    TG.TG_local().all_reduce_in_place_n(to_address(Ov.origin()),nw,std::plus<>());
  }

  // Computes back propagated 1RDM.
  // G is computed locally on each core and needs to be reduced.
  template<class WlkSet, class MatG, class CVec>
  void NOMSD::BackPropagatedDensityMatrix(const WlkSet& wset, MatG& G, CVec &denom, bool path_restoration, bool free_projection)
  {
    using std::fill_n;
    using std::copy_n;
    // temporary runtime check for incompatible memory spaces
    {
      auto dev_ptr_(make_device_ptr(G.origin()));
      auto dev_ptr(make_device_ptr(denom.origin()));
    }
    const int nwalk = wset.size();
    const int ndet = ci.size();
    bool compact = false;
    auto Gsize = size_t(dm_size(not compact));
    if(localGbuff.size() < 2*Gsize+1)
      localGbuff.reextent(iextensions<1u>{2*Gsize+1});
    pointer ov_(localGbuff.origin()+2*Gsize);
    auto wlk_dims = wset.walker_dims();
    if((T2ForBP.size(0) != wlk_dims.first || T2ForBP.size(1) != wlk_dims.second)) {
      T2ForBP.reextent({wlk_dims.second,wlk_dims.first});
      T1ForBP.reextent({NAEA,wlk_dims.first});
      T3ForBP.reextent({wlk_dims.first,wlk_dims.second});
    }
    if(walker_type != COLLINEAR) {
      auto Gdims = dm_dims(not compact,Alpha);
      // Temporaries for determinant component of walker rdm function.
      CMatrix_ref G2D_(localGbuff.origin(), {Gdims.first,Gdims.second});
      CVector_ref G1D_(localGbuff.origin(), iextensions<1u>{Gsize});
      // Accumulator for trial wavefunction sum.
      CVector_ref G21D_(localGbuff.origin()+Gsize,iextensions<1u>{Gsize});
      // 1D view of walker averaged RDM
      CVector_ref G1D(make_device_ptr(G.origin()),iextensions<1u>{Gsize});
      for(int iw=0; iw<nwalk; iw++) {
        if( iw%TG.TG_local().size() != TG.TG_local().rank() ) continue;
        if( std::abs(ComplexType(*wset[iw].weight())) < 1e-10 ) continue;
        fill_n(G21D_.origin(), G21D_.num_elements(), ComplexType(0.0));
        ComplexType overlap = ComplexType(0.0,0.0);
        ComplexType factor;
        if(path_restoration) {
          // Path restoration
          factor = *wset[iw].BPWeightFactor();
        } else if(free_projection) {
          // Free projection.
          factor = *wset[iw].phase();
        }
        else {
          // Phaseless
          factor = ComplexType(1.0,0.0);
        }
        ComplexType wfac = (*wset[iw].weight()) * factor;
        ComplexType trace = ComplexType(0.0,0.0);
        for(int id=0; id<ndet; id++) {
          // We construct PsiBP^{\dagger}
          CMatrix_ref PsiBP = CMatrix_ref(T2ForBP.data(), {NAEA,NMO});
          ComplexType detR = BackPropagateOrbMat(OrbMats[id], wset[iw], PsiBP);
          // Construct RDM with Slater Matrix from beginning of path.
          SDetOp.MixedDensityMatrix(PsiBP,wset[iw].SlaterMatrixN(Alpha),
                                                     G2D_,to_address(ov_),compact);
          ComplexType ov(*ov_);
          if(walker_type==CLOSED) {
            ov *= ov;
            detR *= detR;
          }
          for(int i = 0; i < G2D_.size(0); i++) trace += G2D_[i][i];
          // By construction detR is real (so no complex conjugation needed).
          ov *= detR * std::conj(ci[id]);
          //std::cout << G2D_[0][0] << " " << ov << " " << wfac << std::endl;
          overlap += ov;
          ma::axpy(ov,G1D_,G21D_);
        }
        //std::cout << "TRACE: " << trace << " " << std::endl;
        ComplexType ov_denom;
        if(free_projection) {
          ov_denom = wfac;
          denom[0] += wfac*overlap;
        } else { 
          ov_denom = wfac/overlap;
          denom[0] += wfac;
        }
        ma::axpy(ov_denom, G21D_, G1D);
        //std::cout << G1D[0] << " " << denom[0] << std::endl;
      }
    } else {
      auto GAdims = dm_dims(not compact,Alpha);
      auto GBdims = dm_dims(not compact,Beta);
      CMatrix_ref GA2D_(localGbuff.origin(),
                                   {GAdims.first,GAdims.second});
      CMatrix_ref GB2D_(localGbuff.origin()+GAdims.first*GAdims.second,
                                   {GBdims.first,GBdims.second});
      CVector_ref GA1D_(localGbuff.origin(),
                                   iextensions<1u>{GAdims.first*GAdims.second});
      CVector_ref GB1D_(localGbuff.origin()+GAdims.first*GAdims.second,
                                   iextensions<1u>{GBdims.first*GBdims.second});
      CVector_ref GA21D_(localGbuff.origin()+Gsize,
                                   iextensions<1u>{GAdims.first*GAdims.second});
      CVector_ref GB21D_(localGbuff.origin()+Gsize+GAdims.first*GAdims.second,
                                   iextensions<1u>{GBdims.first*GBdims.second});
      CVector_ref GA1D(make_device_ptr(G.origin()), iextensions<1u>{GAdims.first*GAdims.second});
      CVector_ref GB1D(make_device_ptr(G.origin())+GAdims.first*GAdims.second,
                       iextensions<1u>{GBdims.first*GBdims.second});
      for(int iw=0; iw<nwalk; iw++) {
        if( iw%TG.TG_local().size() != TG.TG_local().rank() ) continue;
        if( std::abs(ComplexType(*wset[iw].weight())) < 1e-10 ) continue;
        ComplexType factor;
        if(path_restoration) {
          factor = *wset[iw].BPWeightFactor();
        } else if(free_projection) {
          factor = *wset[iw].phase();
        } else {
          factor = ComplexType(1.0,0.0);
        }
        ComplexType wfac = (*wset[iw].weight()) * factor;
        ComplexType overlap = ComplexType(0.0,0.0);
        for(int id=0; id<ndet; id++) {
          CMatrix_ref PsiBPAlpha = CMatrix_ref(T2ForBP.data(), {NAEA,NMO});
          ComplexType detR = BackPropagateOrbMat(OrbMats[2*id], wset[iw], PsiBPAlpha);
          SDetOp.MixedDensityMatrix(PsiBPAlpha,wset[iw].SlaterMatrixN(Alpha),
                                                      GA2D_,to_address(ov_),compact);
         
          ComplexType ov(*ov_);
          CMatrix_ref PsiBPBeta = CMatrix_ref(T2ForBP.data()+NMO*NAEA, {NAEB,NMO});
          detR *= BackPropagateOrbMat(OrbMats[2*id+1], wset[iw], PsiBPBeta);
          SDetOp.MixedDensityMatrix(PsiBPBeta,wset[iw].SlaterMatrix(Beta),
                                           GB2D_,to_address(ov_),compact);
          ComplexType ov2(*ov_);
          ov *= ov2*std::conj(ci[id]);
          overlap += ov;
          ma::axpy(ov,GA1D_,GA21D_);
          ma::axpy(ov,GB1D_,GB21D_);
        }
        ComplexType ov_denom;
        if(free_projection) {
          ov_denom = wfac;
          denom[0] += wfac*overlap;
        } else { 
          ov_denom = wfac/overlap;
          denom[0] += wfac;
        }
        ma::axpy(ov_denom, GA21D_, GA1D);
        ma::axpy(ov_denom, GB21D_, GB1D);
      }
    }
    // Average over walker's
    TG.Global().all_reduce_in_place_n(to_address(denom.origin()),1,std::plus<>());
    TG.Global().all_reduce_in_place_n(to_address(G.origin()),Gsize,std::plus<>());
    auto denominator= ComplexType(1.0,0.0)/denom[0];
    CVector_ref GFull(make_device_ptr(G.origin()), iextensions<1u>{Gsize});
    if(!free_projection) {
      ma::scal(denominator,GFull);
    }
    TG.local_barrier();
  }

  /*
   * Orthogonalizes the Slater matrices of all walkers in the set.  
   * Options:
   *  - bool importanceSamplingt(default=true): use algorithm appropriate for importance sampling. 
   *         This means that the determinant of the R matrix in the QR decomposition is ignored.
   *         If false, add the determinant of R to the weight of the walker. 
   */
  template<class WlkSet>
  void NOMSD::Orthogonalize(WlkSet& wset, bool impSamp) {
    if(localGbuff.size() < 1)
      localGbuff.reextent(iextensions<1u>{1});
    pointer d_(localGbuff.origin());
    ComplexType detR(1.0,0.0);
    if(walker_type != COLLINEAR) {
      int cnt=0;
      for(typename WlkSet::iterator it=wset.begin(); it!=wset.end(); ++it) {
        if( (cnt++)%TG.getNCoresPerTG() == TG.getLocalTGRank() ) {
          if(excitedState && numExcitations.first>0)
            OrthogonalizeExcited(it->SlaterMatrix(Alpha),Alpha);
          else { 
            SDetOp.Orthogonalize(it->SlaterMatrix(Alpha),to_address(d_));
            if(!impSamp) detR = ComplexType(*d_);
          }
          if(!impSamp) {
            if(walker_type==CLOSED)
              *it->weight() *= (detR*detR);
            else  
              *it->weight() *= detR;
          }
        }
      } 
    } else {
      int cnt=0;
      for(typename WlkSet::iterator it=wset.begin(); it!=wset.end(); ++it) {
        if( (2*(cnt++))%TG.getNCoresPerTG() == TG.getLocalTGRank() ) {
          if(excitedState && numExcitations.first>0)
            OrthogonalizeExcited(it->SlaterMatrix(Alpha),Alpha);
          else {
            SDetOp.Orthogonalize(it->SlaterMatrix(Alpha),to_address(d_));
            if(!impSamp) detR = ComplexType(*d_);
          }
          if(!impSamp) {
            std::lock_guard<shared_mutex> guard(*mutex);
            *it->weight() *= detR;
          }
        }
        if( (2*(cnt++)+1)%TG.getNCoresPerTG() == TG.getLocalTGRank() ) {
          if(excitedState && numExcitations.second>0)
            OrthogonalizeExcited(it->SlaterMatrix(Beta),Beta);
          else {
            SDetOp.Orthogonalize(it->SlaterMatrix(Beta),to_address(d_));
            if(!impSamp) detR = ComplexType(*d_);
          }
          if(!impSamp) {
            std::lock_guard<shared_mutex> guard(*mutex);
            *it->weight() *= detR;
          }
        }
      }
    }   
    TG.local_barrier();    
    // recalculate overlaps
    Overlap(wset);
  }

  /*
   * Orthogonalize extended Slater Matrix for excited states calculation
   * Ret 
   */
  template<class Mat>
  void NOMSD::OrthogonalizeExcited(Mat&& A, SpinTypes spin)
  {
    if(localGbuff.size() < 1)
      localGbuff.reextent(iextensions<1u>{1});
    pointer d_(localGbuff.origin());
    if(walker_type == NONCOLLINEAR)  
      APP_ABORT(" Error: OrthogonalizeExcited not implemented with NONCOLLINEAR.\n");
    if(spin==Alpha) {
      if(extendedMatAlpha.size(0) != NMO || extendedMatAlpha.size(1) != maxOccupExtendedMat.first)
        extendedMatAlpha.reextent({NMO,maxOccupExtendedMat.first});
      extendedMatAlpha(extendedMatAlpha.extension(0),{0,NAEA}) = A;
      extendedMatAlpha(extendedMatAlpha.extension(0),{NAEA+1,maxOccupExtendedMat.first}) =
                excitedOrbMat[0](excitedOrbMat.extension(1),{NAEA+1,maxOccupExtendedMat.first}); 
      // move i->a, copy trial orb i  
      for(auto& i:excitations) 
        if(i.first < NMO && i.second < NMO ) {
          extendedMatAlpha(extendedMatAlpha.extension(0),i.second) = extendedMatAlpha(extendedMatAlpha.extension(0),i.first);
          extendedMatAlpha(extendedMatAlpha.extension(0),i.first) = excitedOrbMat[0](excitedOrbMat.extension(1),i.first);
        }
      ComplexType detR;
      //SDetOp.Orthogonalize(extendedMatAlpha,std::addressof(detR));
      SDetOp.Orthogonalize(extendedMatAlpha,to_address(d_));
      A(A.extension(0),{0,NAEA}) = extendedMatAlpha(extendedMatAlpha.extension(0),{0,NAEA});  
      for(auto& i:excitations) 
        if(i.first < NMO && i.second < NMO ) 
          A(A.extension(0),i.first) = extendedMatAlpha(extendedMatAlpha.extension(0),i.second);
    } else {
      if(extendedMatBeta.size(0) != NMO || extendedMatBeta.size(1) != maxOccupExtendedMat.second)
        extendedMatBeta.reextent({NMO,maxOccupExtendedMat.second});
      extendedMatBeta(extendedMatBeta.extension(0),{0,NAEB}) = A;
      extendedMatBeta(extendedMatBeta.extension(0),{NAEB+1,maxOccupExtendedMat.second}) = 
                excitedOrbMat[1](excitedOrbMat.extension(1),{NAEB+1,maxOccupExtendedMat.second});
      // move i->a, copy trial orb i  
      for(auto& i:excitations) 
        if(i.first >= NMO && i.second >= NMO ) {
          extendedMatBeta(extendedMatBeta.extension(0),i.second) = extendedMatBeta(extendedMatBeta.extension(0),i.first);
          extendedMatBeta(extendedMatBeta.extension(0),i.first) = excitedOrbMat[1](excitedOrbMat.extension(1),i.first);
        }
      ComplexType detR;
      //SDetOp.Orthogonalize(extendedMatBeta,std::addressof(detR));
      SDetOp.Orthogonalize(extendedMatBeta,to_address(d_));
      A(A.extension(0),{0,NAEB}) = extendedMatBeta(extendedMatBeta.extension(0),{0,NAEB});
      for(auto& i:excitations) 
        if(i.first >= NMO && i.second >= NMO ) 
          A(A.extension(0),i.first) = extendedMatBeta(extendedMatBeta.extension(0),i.second);
    }
  }  

  /*
   * Calculate mean field expectation value of Cholesky potentials
   */
  template<class Vec>
  void NOMSD::vMF(Vec&& v) {
    using std::fill_n;
    using std::copy_n;
    // temporary runtime check for incompatible memory spaces
    {
      auto dev_ptr_(make_device_ptr(v.origin()));
    }

    assert(v.num_elements() == local_number_of_cholesky_vectors());
    fill_n(v.origin(),v.num_elements(),ComplexType(0));  

    int ndets = ci.size(); 
    ComplexType ovlp=0.0;
    CMatrix PsiT,PsiTB;
 
    using std::conj;
    if(ndets == 1) {
      CMatrix G;
      size_t Gsize = dm_size(false);
      if(walker_type != COLLINEAR) {
        auto Gdims = dm_dims(false,Alpha);
        G.reextent({Gdims.first,Gdims.second});
        csr::CSR2MA('H',OrbMats[0],PsiT); 
        ComplexType ov;
        SDetOp.MixedDensityMatrix(OrbMats[0],PsiT,G,std::addressof(ov),true);
      } else {
        G.reextent({NAEA+NAEB,NMO});
        csr::CSR2MA('H',OrbMats[0],PsiT);
        ComplexType ov, ov2;
        SDetOp.MixedDensityMatrix(OrbMats[0],PsiT,
                                       G.sliced(0,NAEA),std::addressof(ov),true);
        csr::CSR2MA('H',OrbMats[1],PsiT);
        SDetOp.MixedDensityMatrix(OrbMats[1],PsiT,
                                       G.sliced(NAEA,NAEA+NAEB),std::addressof(ov2),true);
        ov *= ov2;
      }    
      CVector_ref G1D(G.origin(),iextensions<1u>{G.num_elements()});
      HamOp.vbias(G1D,std::forward<Vec>(v));
    } else {
      size_t Gsize = dm_size(true);
      auto Gdims = dm_dims(true,Alpha);
      CVector G1D(iextensions<1u>{Gsize});
      fill_n(G1D.origin(),G1D.num_elements(),ComplexType(0));
      ComplexType ov(0);  
      if(walker_type != COLLINEAR) {
        CMatrix G_({Gdims.first,Gdims.second});
        CVector_ref G1D_(G_.origin(),iextensions<1u>{Gdims.first*Gdims.second});
        int n0,n1;
        std::tie(n0,n1) = FairDivideBoundary(TG.getGlobalRank(),ndets*(ndets+1)/2,
                                             TG.getGlobalSize());
        int last_p=-1;
        for(int p=0, pq=0; p<ndets; p++) {
          for(int q=p; q<ndets; q++, pq++) {
            if(pq < n0) continue;
            if(pq >= n1) break;
            if(last_p != p) {
              last_p=p;
              csr::CSR2MA('H',OrbMats[p],PsiT);
            }
            ComplexType ov_;
            SDetOp.MixedDensityMatrix(OrbMats[q],PsiT,G_,std::addressof(ov_),false);
            if(walker_type==CLOSED) ov_ *= ov_;
            ov += real(conj(ci[q])*ci[p]*ov_);
            for(int i=0; i<G1D.num_elements(); i++)
              G1D[i] += real(conj(ci[q])*ci[p]*ov_*G1D_[i]);
          }
        }
      } else {
        CMatrix G_({2*NMO,NMO});
        CVector_ref G1D_(G_.origin(),iextensions<1u>{2*NMO*NMO});
        int n0,n1;
        std::tie(n0,n1) = FairDivideBoundary(TG.getGlobalRank(),ndets*(ndets+1)/2,
                                             TG.getGlobalSize());
        int last_p=-1;
        for(int p=0, pq=0; p<ndets; p++) {
          for(int q=p; q<ndets; q++, pq++) {
            if(pq < n0) continue;
            if(pq >= n1) break;
            if(last_p != p) {
              last_p=p;
              csr::CSR2MA('H',OrbMats[2*p],PsiT);
              csr::CSR2MA('H',OrbMats[2*p+1],PsiTB);
            }
            ComplexType ov_, ov2_;
            SDetOp.MixedDensityMatrix(OrbMats[2*q],PsiT,
                                G_.sliced(0,NMO),std::addressof(ov_),false);
            SDetOp.MixedDensityMatrix(OrbMats[2*q+1],PsiTB,
                                G_.sliced(NMO,2*NMO),std::addressof(ov2_),false);
            ov += real(conj(ci[q])*ci[p]*ov_*ov2_);
            for(int i=0; i<G1D.num_elements(); i++)
              G1D[i] += real(conj(ci[q])*ci[p]*ov_*G1D_[i]);
          }
        }
      }  
      TG.Global().all_reduce_in_place_n(to_address(G1D.origin()),G1D.num_elements(),std::plus<>());  
      ComplexType ov_ = ( TG.Global() += ov );
      ma::scal(ComplexType(1.0,0.0)/ov_,G1D);
      HamOp.vbias(G1D.sliced(0,Gdims.first*Gdims.second),
                  std::forward<Vec>(v));
      if(walker_type==COLLINEAR)
        HamOp.vbias(G1D.sliced(NMO*NMO,2*NMO*NMO),
                  std::forward<Vec>(v),1.0,1.0);
    }
    // since v is not in shared memory, we need to reduce
    TG.TG_local().all_reduce_in_place_n(to_address(v.origin()),v.num_elements(),std::plus<>());
    // NOTE: since SpvnT is a truncated structure the complex part of vMF, 
    //       which should be exactly zero, suffers from truncation errors.
    //       Set it to zero.
    for(int i=0; i<v.num_elements(); i++)
      v[i] = ComplexType(real(v[i]),0.0); 
  }

  template <class MatA, class Wlk, class MatB>
  ComplexType NOMSD::BackPropagateOrbMat(MatA& OrbMat, const Wlk& walker, MatB& PsiBP)
  {
    // temporary runtime check for incompatible memory spaces
    {
      auto dev_ptr_(make_device_ptr(PsiBP.origin()));
    }
    ComplexType detR = ComplexType(1.0, 0.0), detR_;
    const int nbp = walker.NumBackProp();
    int ip = nbp - 1;
    //boost::multi::array_ref<const ComplexType, 2> B = walker.BMatrix(ip);
    auto B(walker.BMatrix(ip));
    ma::product(OrbMat, B, T1ForBP);
    for (int i = 0; i < nbp-1; i++) {
      ip -= 1;
      auto B(walker.BMatrix(ip));
      //boost::multi::array_ref<const ComplexType, 2> B = walker.BMatrix(ip);
      ma::product(T1ForBP, B, PsiBP);
      if ((i != 0) && (i%10 == 0)) {
        ma::transpose(PsiBP, T3ForBP);
        SDetOp.Orthogonalize(T3ForBP,std::addressof(detR_));
        detR *= detR_;
        ma::transpose(T3ForBP, PsiBP);
      }
      T1ForBP = PsiBP;
    }
    return detR/1e6;
  }

}

}
